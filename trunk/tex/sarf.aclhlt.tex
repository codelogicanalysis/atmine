% File naaclhlt2010.tex
% Contact: nasmith@cs.cmu.edu
\documentclass[11pt]{article}
\usepackage{acl-hlt2011}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\usepackage{arabtex}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{color}
\usepackage{rotate}
\usepackage{rotating}
\usepackage{amsthm}
\usepackage{booktabs}

\usepackage{relsize}
\usepackage{fancyvrb}
\usepackage[colorlinks=false]{hyperref}

\usepackage{utf8}
\setarab
\fullvocalize
\transtrue
\arabtrue


%\newcommand{\CharCodeIn}[1]{`\CodeIn{#1}'}
\newcommand{\CodeIn}[1]{{\small\texttt{#1}}}
\newcommand{\frl}[1]{\fbox{\RL{#1}}} 
\newcommand{\noArRL}[1]{\arabfalse\RL{#1}\arabtrue} 
\newcommand{\noTrRL}[1]{\transfalse\RL{#1}\transtrue} 

%\title{Instructions for NAACL HLT 2010 Proceedings\Thanks{This...}}
\title{Application Specific Arabic Morphological Analyzer for Arabic Text Mining}

%\author{ Jad Makhlouta \\\And
%Hamza Harkous \\
%  American University of Beirut \\
%  {\tt \{jem04, hhh20, fz11\}@aub.edu.lb } \\\And 
%  Fadi Zaraket 
%}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Natural language processing (NLP) uses morphological 
analysis to abstract and annotate text.
Often times, 
%      and in particular in the context of the Arabic language, 
annotations resulting from 
a branch of morphological analysis may not be
appropriate for the case study at hand. 
In this paper we present Sarf, a case-based morphological 
analyzer for Arabic. 
Sarf allows a case-based controller to control and refine
the state of the analysis on the fly. 
It uses non-deterministic finite state Automata
to analyze a stream of text. 
%Each machine corresponds to a valid analysis. 
Sarf uses concatenative analysis based on recursive affixes 
to better preserve part of speech information.
We automated the analysis of three books of Islamic
narrations (hadith) using Sarf where we
abstracted each book into a vector of complex structures.
Our results show higher accuracy and better efficiency
compared to current analyzers.
\end{quote}
\end{abstract}

\section{Introduction}

%Automated analysis of Arabic data sets, including texts, 
%publications, records and digital media is essential
%with the huge digital Arabic content available nowadays. 
Morphological analysis is key in current automated 
analysis techniques for Arabic text. 
Current morphological analyzers~\cite{Sughaiyer:04}
use concatenative analysis when
considering the internal structure 
of an Arabic word and
composing it into several {\em morphemes}. 
A morpheme can be a {\em stem}, or an {\em affix}.
An affix can be a {\em prefix, suffix, } or an {\em infix}.
The analysis of one word may lead to several possible
morphological solutions.
\vocalize
For instance, the word \RL{'a.hmadH}~\footnote{In this document, we use the default ArabTeX transliteration style ZDMG.}
may have two valid morphological analyses. 
The letter \RL{'a} may be a prefix and the word means 
``I praise him'', or 
%The letter \RL{'a} may also be 
part of the stem \RL{'a.hmad} (a proper noun)
and the word means ``his Ahmad''.

\novocalize
The accuracy of the solutions suffer due to inherent difficulties
of morphological analysis of the Arabic language. 
For example, it is common practice to write Arabic text
without short vowels. 
This greatly increases the ambiguity of Arabic text. 
Arabic letters can have up to %(maybe reduce this sentence)
four different forms
corresponding to their position in a word, i.e, beginning,
middle, end of word and separate forms. 
This allows the phrase \transfalse
\RL{il_A\nospace almdrsT} \transtrue
to be visually recognizable
as two separate words \RL{il_A} (to) and \RL{almdrsT} (the school) 
without the need of a space in between. 
The reason is the first word \RL{il_A} ends with
\RL{_A} a non-connecting letter. 
These words,
referred to as ``run-on'' words~\cite{Buckwalter:04},
occur often, and greatly increase the
difficulty of tokenization.

{\bf Sarf.~~~}
In this paper, we present Sarf~\footnote{Sarf (name modified for blind review) 
has been submitted with the paper and will be available online as an open source tool.},
 a {\em novel case-based efficient
morphological analyzer} that uses 
non-deterministic finite state Automata 
driven by an application-specific controller.
Each machine in Sarf takes one letter at a time as input
and represents a valid morphological analysis.
The application-specific controller prunes false positives
early in the run by making a decision at every input letter
to control the machines.
The controller we present in Section~\ref{sec:controller}
is an FSA. However, as we explain in Section~\ref{sec:ndfsa},
it can be a logical entity of any type. 

%Each alive machine in Sarf takes as input one letter at a time 
%from a text stream. 
Sarf does not assume that the word at hand is a token and
performs tokenization on the fly based on morphological correctness.
Up to our knowledge, this is the first morphological analyzer that 
handles the ``run-on'' words problem. 

Sarf uses concatenative analysis with a novel 
{\em recursive affixes} refinement,
where an affix can be a sequence of other affixes which are concatenatively compatible.
%An atomic affix $a$ is a standalone affix
%that is not the concatenation of other affixes. 
%We define an affix $f: a | a f$ to be either
%an atomic affix $a$ or an atomic affix concatenated
%with an affix. 
Sarf introduces recursive affixes in order to
better retain part of speech information and enhance the 
efficiency of affix matching. 


Our main focus when building Sarf, was not on improving the semantical rules and expanding the morphological alternatives, instead we worked on adding support for higher level control by the specific application utilizing the lower level morphological analyzer. In other words, application-specific controller, has full control over and directs the morphological analysis of Sarf. % (sentence should be reworded differently, but is needed to explain why we mention higher and lower accuracy hypothesis, which seems vague otherwise)

We hypothesize that many NLP case studies need the 
morphological analyzer to answer queries that do not need 
high accuracy at the low morphological analysis level.
The analysis at the higher levels 
(i.e. answering the application-specific query)
can often compensate for 
tolerable inaccuracy at lower morphological levels. 
For example, if a query concerns proper names and the 
analyzer is considering a 
prefix that connects only to a verb,
%the analyzer can answer without considering the 
%rest of the word.
we do not need to further analyze the rest of the word 
to provide a negative answer.

We find evidence to our hypothesis in~\cite{Maamouri:10}.
The addition of a new corpus to the Arabic Tree Bank 
(ATB)~\cite{Maamouri:04}
%required compliance with the DARPA GALE program 
%including several NLP tasks such as English translation 
%annotations and word-level alignment of Arabic and English. 
%The new data 
challenged the existing processing and 
annotation guidelines. 
This led to refining 
SAMA~\cite{Kulick:10} with 
alternative guidelines. 
This refinement was achieved only after the NLP 
task interacted with the morphological analyzer,
a flexibility our analyzer is designed to provide for the application-specific controller on the run. %(check if this sentence makes sense)
In addition, 
the work of Habash~\shortcite{Habash:06} reports that different 
types of morphological analyses behave differently on the same %case study. 
data set.
The analyzers may report different solutions or 
may present the solutions in different orders. 
Clearly, some of 
the presented solutions and their order of presentation may not be 
appropriate for the %case study.
application needs, which is an issue that Sarf tackles. % (maybe reworded)
Moreover, our results strongly support our claim that
an application-specific morphological analyzer that allows a application-specific 
controller to intervene at every decision to guide, use, order, and 
refine the morphological analysis is of high utility.
We validate our hypothesis using a case study from the Islamic 
literature, introduced later in Section~\ref{sec:islamic}.

The rest of this paper is structured as follows. In Section~\ref{sec:related}
we compare Sarf to related work. In Section~\ref{sec:sarf}
we discuss Sarf and illustrate it with a running example.
%We also explain how non-deterministic finite state Automata
%work. 
In Section~\ref{sec:islamic} we present the case
study. We present our results in Section~\ref{sec:results}
and conclude with future work in Section~\ref{sec:future}.

% does not have to be a subsection, 
%just to highlight it as an item we should not miss

%\section{Background}
% TODO: nondeterministic finite-state automata

\section{Related work }
\label{sec:related}

The survey in~\cite{Sughaiyer:04} compares
several morphological analyzers. 
Analyzers such as~\cite{Khoja:01,Darwish:02} 
target specific applications in the 
analyzer itself or use a specific set of POS tags
as their reference.
Sarf differs in that it is a general morphological 
analyzer that reports all possible solutions. 
It is application-specific in the sense that a controller 
prunes these solutions. 

Current morphological analyzers such as 
Buckwalter~\shortcite{Buckwalter:02},
Beesley~\shortcite{Beesley:01},
SAMA~\cite{Kulick:10},
and ElixirFM~\cite{Otakar:07} 
take as input white space delimited tokens~\cite{Kulick:10},
consider them as words,
and enumerate all possible solutions. 
%This approach has several problems. 
However, we note the following points that deserve consideration:
%(We need not to mention especially point 2 as a problem according to reviewer comments)
First, a white space delimited token may have 
more than one word.
%Second, the exhaustive enumeration may hurt performance and may
%not be necessary or appropriate
%in some case studies as noted in~\cite{Maamouri:10}. 
Second, in many applications, the exhaustive enumeration may not be needed or appropriate as noted in~\cite{Maamouri:10}; consequently, adding unnecessary running time.
Other morphological analyzers such as 
Amira~\cite{Diab:07,Benajiba:07},
MAGEAD~\cite{Habash:05}, and MADA+TOKAN~\cite{Habash:09} 
% (reviewer wants to distinguish MADA as a tool for morphological disambiguation instead of analysis)
use machine learning and support vector machines (SVM) 
to enhance the accuracy of the morphological analysis at the expense 
of performance. % (we should stress that we mean performance in time and not accuracy although evident since some reviewers did not get)
Xerox rules can be compiled into specialized finite state
machine (FSM) based analyzers as described in~\cite{Beesley:03}.
However, the efficiency of the resulting analyzer depends on the
way the Xerox rules are written. 
This requires deep knowledge and insight from the user
in compilation techniques, context free grammars, 
and morphological analysis.
%context free grammar rules that can be automatically compiled
%into efficient non-deterministic finite state Automata (NDFSA). 



Sarf builds upon the lexicon of Buckwalter\shortcite{Buckwalter:02}.
It differs from Buckwalter in that it defines a shorter list of affixes
and a longer list of affix compatibility rules to allow 
affixes to be defined recursively
so that we can have prefix-prefix an suffix-suffix 
concatenations.
This allows Sarf to better maintain and propagate 
the POS tags associated with affixes. 
%This information is highly 
%redundant with Buckwalter.% and that may lead to consistency issues. 
Sarf keeps all the possible analyses alive where each analysis
corresponds to a set of decisions and positions in the text. 
Buckwalter produces a set of solutions for each token 
and the several analysis threads of the text need to be 
generated as a product of the token solutions. 

Similar to Sarf, Beesley~\shortcite{Beesley:01} uses
finite state Automata (FSA). 
It reports that the number of Automata generated by a compiler
for Xerox rules can not be controlled and also reports a difficulty 
to compose the FSAs into a single framework. 
Sarf however constructs an NDFSA from
the controller and the prefix, stem, and suffix deterministic 
transducers to solve the problem.
Sarf also differs in that it uses recursive affixes in the 
affix transducers, and in that the transducers consider
input from the controller to modify their behavior. 
%With Sarf it is easy to control the FSA with a case-based
%controller. 
Beesley requires recompilation of the Xerox rules. % (don't we do also for any change in the controller code?)

Similar to The MADA+TOKAN toolkit, Sarf thinks of
the several valid morphological analyses as a richness that 
should be exploited at a higher abstract level rather than
an inaccuracy that should be corrected. 
Sarf differs in that it provides an interface for the 
case study to interfere in judging the analyses much earlier and
at a finer granularity level. 
In Sarf, there is no need for a separate tokenizer such as
TOKAN since each solution keeps a stack of positions
that partition text into morphemes.
%The case-based controller can build the trace 
%when needed. 

%MAGEAD 
%Maamouri, Mohamed and \bgroup et al.\egroup },

Our approach is close to the refinement of SAMA~\cite{Maamouri:10}.
SAMA was refined to interact with
the ATB~\cite{Maamouri:04} project after the addition of a large 
new corpus. 
%We find supporting evidence in this decision to our hypothesis
%about the utility of a case-based analyzer. 
The algorithmic changes in SAMA were
done manually and worked in integration with the ATB format. 
Sarf is an application-specific controller approach that can modify 
the behavior of the analyzer on the fly and can interact
with any case study. 
%In Sarf, the analyzer remains general, and the 
%case-based controller specializes the analysis.

The AMIRA~\cite{Diab:07} analyzer uses 
a language independent SVM based analysis. 
The SVM analysis learns features from the input text 
and does not make use of any information 
in the target query.

Like ElixirFM~\cite{Otakar:07}, Sarf builds on the lexicon
of the Buckwalter analyzer. 
Sarf also uses deterministic parsing with tries 
to implement the affix and stem transducers. 
We think that the inferential-realizational approach 
of ElixirFM
that is highly compatible with the Arabic linguistic 
description~\cite{Badawi:04}
can benefit from many features unique to the Arabic language.
Sarf leaves implementing that to the application-specific controller
since in many cases the abstraction needs only a partial 
linguistic model of the Arabic language. 
%We can compare the case-based controller in Sarf to a high level
%component in the ElixirFM domain specific language. 



\section{Sarf}
\label{sec:sarf}

Sarf extends the lexicon of Buckwalter~\shortcite{Buckwalter:02} with
proper names and location names extracted from different online 
sources~\footnote{\href{http://alasmaa.net/}{http://alasmaa.net/ }, 
\href{http://ar.wikipedia.org/}{http://ar.wikipedia.org/}}
as well as biblical sources~\footnote{Genesis 4:17-23; 5:1-32; 9:28-10:32; 11:10-32; 25:1-4, 12-18; 36:1-37:2; Exodus 6:14-25; Ruth 4:18-22; 1 Samuel 14:49-51; 1 Chronicles 1:1-9:44; 14:3-7; 24:1; 25:1-27:22; Nehemiah 12:8-26; Matthew 1:1-16; Luke 3:23-38}.

Sarf encodes the prefix, suffix and stem lexicons into
three deterministic transducers and 
uses a non-deterministic construction
of the three to compute all valid morphological analyses.
We first present a running example of Sarf.
The running example explains how Sarf works,
implements recursive affixes, and
handles ``run-on'' words.
%Then we discuss how Sarf builds and composes the different FSAs.

\subsection{Sarf example}
\label{sec:example}

\transfalse
\begin{figure}[tb]
\center{
\resizebox{\columnwidth}{!}
{ \input{figs/FSM.pdftex_t}}
\caption{Example affix and stem transducers.}
\label{f:example}
}
\end{figure}
% the FSMs for the three words
\transtrue

%{\bf Sarf example.}

The diagram in Figure~\ref{f:example}
illustrates the operation of $\Phi$, the Sarf FSA,
when parsing the
string \RL{wsyl`b-h-aa al-lA`bwn}.
%\RL{al-lA`bwn}\RL{wsyl`b-h-aa}.
\footnote{ 
\RL{n}%
\RL{w}
\RL{b}
\RL{`}
\RL{a} 
\RL{l}
\RL{l}
\RL{a} %
\RL{a}
\RL{h}
\RL{b}
\RL{`}
\RL{l}
\RL{y}
\RL{s}
\RL{w}
in separate form to ease following the example
}

Boxes and circles denote accept and regular
states respectively.
The edges are transitions with labels corresponding to
the input letters.
The reject states are omitted for clarity and are denoted
by the absence of corresponding transitions. 

Subfigures (a), (b), and (c) in Figure~\ref{f:example}
represent ${\cal P}$ the prefix ,
${\cal S}$ the stem, and ${\cal X}$ the suffix
transducers respectively. 
The symbol $\epsilon$ represents an empty string and is 
the source of non-determinism.

We start with $\Phi$ at the root square state in 
${\cal P}$ which is an accept state. 
The $\epsilon$-edge connects ${\cal P}$
to ${\cal S}$ and transitions from any accept state
in ${\cal P}$ to the root state in ${\cal S}$.
The $\epsilon$-edge that connects ${\cal S}$ to ${\cal X}$
follows the same behavior. 

When there are two valid transitions such as \RL{w} 
and $\epsilon$ in the start case, 
$\Phi$ spawns an exact copy of itself $\Psi$. 
$\Phi$ makes one transition, and $\Psi$ makes the other. 
Each of $\Phi$ and $\Psi$ represent a valid analysis so far. 
An FSA ($\Phi$ or $\Psi$) dies when it reaches a reject state.
In our example, if there were no stems that start 
with the letter \RL{w}, $\Psi$ will die. 
In reality, $\Psi$ will die when the input 
is at \RL{wsyl`}. 

Note that the affix transducers allow recursive 
affixes. 
For example, \RL{w} will result in an accept state
that transitions to ${\cal S}$.
When \RL{s} follows, we move to another accept state in 
${\cal P}$ corresponding to the prefix \RL{ws}. 
The same applies to ${\cal X}$. 

Lets consider $\Phi$ after it consumed \RL{wsy} 
and transitioned into the root node of ${\cal S}$.
Now $\Phi$ will transition with \RL{l`b} to reach an accept 
state. 

Before moving with the letter \RL{b} to the accept state,
$\Phi$ needs to make sure that the stem \RL{l`b} is compatible
with the prefix \RL{wsy}. 
Sarf keeps compatibility category values as part
of the accept states. 
%Sarf considers the value of the category as part of the state.
Thus each accept state in Figure~\ref{f:example} represents
more than one concrete state. 
If the category of \RL{l`b} is compatible with the category of
\RL{wsy} then \RL{b} will move $\Phi$ to an accept state. 
Otherwise, it will move it to a regular or a reject state. 
       
Since $\Phi$ is now in an accept state in ${\cal S}$, it 
spawns $\Xi$ which transitions to the root of ${\cal X}$. 
After it consumes \RL{h--} $\Phi$ dies since there is no
\noTrRL{h--}-edge from the current state.
$\Xi$ represents a valid analysis and proceeds.
       
Note that $\Xi$ reports a full word at any accept state
by spawning a new FSA using the $\epsilon$ transition
to ${\cal P}$.
In Sarf, this only happens with white space, delimiters, 
and non-connecting letters. 
We left that out in Figure~\ref{f:example} for clarity. 
       
The case-based controller can interfere at any point in the 
analysis to make a decision like ignoring an FSA that 
is not interesting. 
It may also decide to correct one FSA
based on the decision of the others. 
The controller uses the output of the transducers as input
to make its decision. 
It does not produce external output and therefore 
the three transducers with the 
controller compose an FSA.

\subsection{Recursive affixes}
\label{sec:recaffix}

We call two morphemes compatible if their concatenation
forms a legal morpheme. 
Buckwalter~\shortcite{Buckwalter:02} keeps separate lists 
of affix and stem morphemes and assigns categories to
morphemes. 
%The relation between morphemes and categories is one 
%to many. 
A compatibility rule is a pair of categories 
$\langle c_1, c_2\rangle$  stating that $c_1$ morphemes
can be concatenated with $c_2$ morphemes.
Buckwalter keeps three lists $L_{ps}, L_{sx},$ and $L_{px}$ 
of compatibility rules relating
prefixes to stems, stems to suffixes, and prefixes to suffixes
respectively. 
Consider a string $s=\alpha\beta\gamma$ where $\alpha$ is 
a prefix, $\beta$ is a stem, and $\gamma$ is a suffix;
$\alpha\beta\gamma$ is a 
valid morphological analysis iff
$\langle c(\alpha),c(\beta)\rangle \in L_{ps}$ and
$\langle c(\beta),c(\gamma)\rangle \in L_{sx}$ and
$\langle c(\alpha),c(\gamma)\rangle \in L_{px}$, where
$c()$ returns the category of the morpheme.

Many affixes are composed of other affixes. For example,
the prefix \RL{wsy} is composed of three other prefixes
namely \RL{w}, \RL{s}, and \RL{y}.
The POS tag for \RL{wsy} is a concatenation
of the POS tags of each of the three morphemes. 
Keeping all valid morphemes in a list
is redundant and may lead to consistency issues when
users try to add their custom morphemes.
We introduce $L_{pp}$ and
$L_{xx}$ as prefix-prefix and suffix-suffix 
compatibility rules respectively.
%This allows us to keep a shorter list of affixes. 
We also introduce a {\em resulting category}
for the $L_{pp}$ and  $L_{xx}$ resulting morphemes.
%The resulting category in all practical cases
%is that of the second morpheme in the pair. 

%Sarf only keeps atomic affixes and their categories.

\subsection{Affix transducers}
\label{sec:affixFSA}

We analyzed the prefixes and suffixes of 
Buckwalter~\shortcite{Buckwalter:02}
and constructed the recursive affixes
into directed acyclic graphs (DAG) similar to 
those in Figures~\ref{f:example}(a) and~\ref{f:example}(c).
The transducers are deterministic and thus 
their traversal is linear.
This is computationally superior to the 
approach of Buckwalter where the analyzer considers
all possible substrings %of the word in question
and looks them up in the affix tables. 

The affix transducers encode $L_{pp}$ and
$L_{xx}$.
An accept state in the transducer corresponds to the last letter 
of an affix in the affix lists.
The accept states produce as output the POS and other tags
associated with the state.

\subsection{Stem transducer}
\label{sec:stemFSA}

We built our stem lexicon using the stem lexicon of 
Buckwalter. 
We augmented the lexicon with a set of proper names and
a set of location names which we 
obtained from online and biblical sources. 

The stems share lots of substrings. We encoded them in
an efficient double array trie structure~\cite{Aoe:89}. 
The structure is deterministic and free of cycles where the 
accept states are the terminal nodes corresponding to the last 
letters in stems. 
This approach is superior to Buckwalter since it consists of
a linear walk in the trie while with Buckwalter we need
a number of hash lookups in the order of all possible partitions
of the string.

\subsection{Construction of Sarf from transducers}
\label{sec:ndfsa}

The diagram in Figure~\ref{f:composition} shows an 
abstraction of the non-deterministic construction of Sarf
from the prefix, stem and suffix transducers. 
The circle represents regular states, the square
represents accept states and the triangle represents
reject states. 
Note that the concrete prefix, stem and suffix transducers
are all deterministic and cycle free as discussed 
is Sections~\ref{sec:affixFSA} and~\ref{sec:stemFSA}.
The cycles are introduced in the abstract machine
for pedagogical purposes.

\begin{figure}[tb]
\center{
\resizebox{\columnwidth}{!}
{ \input{figs/abstract_machine.pdftex_t}}
\caption{Sarf NDFSA.}
\label{f:composition}
}
\end{figure}

The algorithm \CodeIn{NDSarf} (detailed below)
takes as input a text string $L$ and a case-based controller
$A$. 
It starts with one machine $\Phi_1$ similar to the one in 
Figure~\ref{f:composition} and inserts it into a collection
$M$. 
The algorithm reads a letter $c$ at a time from $L$
and iterates over all machines in $M$. 
For each $\Phi$ in $M$,
if $\Phi$ is in an accept state and it is in the suffix
phase, then the algorithm checks whether it should report
a full word. 
This happens when $c$ is a white space, a delimiter, 
or when the preceding letter %leading to the current state
was a non-connecting letter. 

In all cases, if $\Phi$ is in an accept state, 
it spawns another machine $\Psi$ and adds it to $M$. 
Notice that spawning a new machine requires only keeping
the state of the machine since all machines share the
structures of the three transducers.
If $c$ leads to a reject state, then $\Phi$ dies 
and we remove it from $M$. 
Otherwise, $\Phi$ transitions using the $c$ edge.
After iterating over all machines in $M$, we invoke the 
controller $A$
and grant it full access and control over
all machines in $M$. 

%The arrows in Figure~\ref{f:composition} that transition
%from prefix to stem, stem to suffix, and suffix to prefix
%require compatibility constraints for 
%the transition.

\begin{table}[tb]
%\centering
%\resizebox{.9\columnwidth}{!}{
\begin{tabular} {p{6cm}}
\begin{Verbatim}[fontsize=\relsize{-2},
frame=topline,framesep=4mm,label=\fbox{NDSarf algorithm},
commandchars=\\\{\}, codes={\catcode`$=3\catcode`_=8}]
NDSarf(string $L$, Controller $A$) 
  MachineVec $M$; -- collection of machines
  Machine $\Phi_1$;
  $M$.insert($\Phi_1$);
  foreach $c$ in $L$ \{
    foreach $\Phi$ in $M$ \{
      if ( $\Phi$.state.isAccept() ) \{
        if ($\Phi$.isSuffix())
          if ($c$.isWhite()) or 
            $A$.report();
          if ($\Phi$.lastChar().isNonConnecting())
            $A$.report();
        $\Psi$ = $\Phi$.clone();
        $M$.insert($\Psi$); \}

      if ( $\Phi$.isWalkable($c$) ) \{
        $\Phi$.transition($c$);
      \} else \{
        $M$.remove($\Phi$);
        $\Phi$.die();
      \} \} 
    $A$.control($M$, $c$); \}
\end{Verbatim}
\end{tabular}
%}
\label{a:ndsarf}
\end{table}

\section{Islamic literature case study}
\label{sec:islamic}


% TODO: check how to merge this part here
\transfalse
\begin{figure}[tb]
\center{
\resizebox{.9\columnwidth}{!}
{ \input{figs/exhadith.pdftex_t}}
\caption{Hadith abstraction example.}
\label{f:exhadith}
}
\end{figure}
% the FSMs for the three words
\transtrue

We validate our hypothesis using a case study from the Islamic 
literature. 
A \RL{.hady_t} is a narration related to the prophet Mohammad
through a \RL{sanad} or a sequence of narrators. 
Figure~\ref{f:exhadith} shows an example \noArRL{.hady_t} in 
Arabic with its transliteration and translation. 
The authenticity of a \noArRL{.hady_t} depends on 
the credibility of the narrators as reported in 
separate biography books. 
In this paper, we consider the problem of automatically segmenting
a \noArRL{.hady_t} book into narrations, each 
narration into its \RL{matn} (content) and its \noArRL{sanad},
and each sanad into the 
separate narrators. Later, we can look each narrator 
up in the biography books. 
For example, 
Figure~\ref{f:exhadith} shows proper names in boxes connected
to form complex names of narrators. 
For example, 
\noTrRL{qtybT} is the first name 
of narrator $n_1$, and 
\noTrRL{s`yd} is the name of his father as 
the word \noTrRL{bn} (son of) indicates. 


The hadith case study is interesting to Sarf since the controller
considers %most of the analyses where we have a concentration
proper names and ignores text where
proper names occur rarely.% as long %as the text is valid.
We are also interested in discovering words 
that relate names to each other such as \RL{bin} or ``the son of''
and words that mean narrate such as \RL{qaal} or ``said''. 

With Sarf, we successfully automated the analysis of 
three books of Islamic narrations~\cite{IbnHanbal,AlTousi,AlKulayni}
and extracted from each one of them a vector of complex structures 
that are three level deep in hierarchy.
%We also showcase that the results of the accurate high level 
%abstraction
%results of the case-based analysis was not affected by a major
%refinement to the analyzer that increased low level analysis accuracy.



A hadith in Islamic literature is a narration from the prophet Mohammad
related by multiple narrators.
Establishing the authenticity of a hadith is an important task
in Islamic studies. 
Based on them Islamic scholars issue rules that affect the life
of people around the world. 
The literature lacks an exhaustive mechanism of authentication
checks. This is currently manual work prone to error and almost
impossible to complete in the life time of a scholar due to the
large literature available. 
%Several researchers~\cite{Hadithopaedia:08} attempted to automate 
%the analysis of the hadith literature. 
In this paper, we use Sarf to successfully automate the
analysis of three books of hadith selected 
arbitrarily~\cite{IbnHanbal,AlKulayni,AlTousi}\footnote{We obtained
  the digitized books from online sources such as 
  \href{http://www.yasoob.com/}{http://www.yasoob.com/} and 
  \href{http://www.al-eman.com/}{http://www.al-eman.com/}. }
%  Our understanding is that those books are historical documents with 
 % public IP.}.


%\setcode{utf8}
%\begin{arabtext}
%حدثنا \emphasize{هشيم}، أخبرنا \emphasize{إسماعيل بن أبي خالد}،
% عن أبي إسحاق، عن سعيد بن جبير، قال كنا
%    مع ابن عمر رضي الله عنه حيث أفاض من
%    عرفات إلى جمع فصلى بنا المغرب ومضى
%\end{arabtext}
%\setcode{standard}

Our analysis accepts a book as input
and segments it into a vector of hadiths. 
We segment each individual hadith into two parts. 
The first part is the sanad which
contains the chain of narrators who related the hadith
from Mohammad. 
The second part is the matn or the content
of the hadith. 
We further explore the sanad and
detect the chain of narrators and 
the relation that links each narrator to his ancestor and 
predecessor in the chain. 
We also detect the full name of each narrator that is
usually composed of several proper names with connectors
in between. 

We built our hadith controller to target
the detection of proper names. 
This includes the task of finding compound names 
composed of several words such as \RL{`bd alr.hmn} often
appearing as ``run-on'' words.
The controller also targets words that mean ``narrate'' when
they appear in the neighborhood of multiple proper names. 

\subsection{Controller}
\label{sec:controller}
% FSM for the controller

We illustrate the hadith controller 
in Figure~\ref{f:hadith}. 
The transitions are labeled by outputs generated from 
the Sarf transducers. 
A NAME label denotes a proper name.
An NRC label denotes a narrator connector such as
\RL{`an} ``on behalf of'', \RL{.hada_t} ``narrated'', \RL{qaal} ``said'', 
\RL{'a_hbar} ``told'' or one of their derivations. 
An IBN label denotes the word \RL{ibn} ``the son of'' and is a name connector.
A NISBA label is a possessive adjective that qualifies a person such 
as \RL{alma.sriy} ``the Egyptian''. 

The symbol $\tau_{\mbox{NMC}}$ is a threshold
that corresponds to the number of tolerated name connectors 
that may occur between two names. 
The symbol LIST$_{\mbox{NMC}}$ corresponds to the list 
of name connectors collected since the controller
started looping in the state NMC\_S. 
The symbol $\lambda_{\mbox{NMC}}$ is a parameter 
that corresponds to a relaxed tolerance measure that
the controller resorts to in case the words separating
two names were longer than $\tau_{\mbox{NMC}}$ but 
contained a name connector word such as IBN or NISBA.

Intuitively, the controller reports a valid chain 
of narrators when a sequence of names
connected by narrator connectors appears. 
It marks the beginning of the hadith with the beginning of 
the current sequence,
and marks the end of the hadith with the beginning of the 
next sequence. 
It also marks the chain of narrators as the sequence itself. 

The controller has four states that correspond to 
a position in text relative to the next sanad. 
State TEXT\_S is the initial state and denotes that
the controller is outside the context of a sanad.
The controller moves to state NAME\_S on
NAME.
State NRC\_S means that the controller is in the context
of a narrator name after Sarf reports an NRC.
State NMC\_S
indicates that the controller expects a name to appear within 
a tolerance threshold expressed by 
$\tau_{\mbox{NMC}}$ and $\lambda_{\mbox{NMC}}$.


Similarly, the NRC\_S state tolerates $\tau_{\mbox{NRC}}$ words 
before it gives up on its expectations. 

We reach the NAME\_S state only when a
valid NAME is detected and we leave when no more NAME's are detected.
The NRC\_S state can only be reached if an NRC is detected.
NMC\_S can only be reached from a NAME\_S state.

\begin{figure}[tb!]
\center{
\resizebox{.8\columnwidth}{!}
{ \input{figs/hadith.pdftex_t}}
\caption{The controller of the hadith case study.}
\label{f:hadith}
}
\end{figure}

%The definition of input labels such as NAME and IBN depends on the 
%morphological analyzer. 
%However, our case-based controller approach was able to perform well
%under both Sarf and a refined version of Sarf.


\section{Results}
\label{sec:results}

\begin{table*}[bt]
\centering
\caption{Results of the hadith case study with Sarf.}
%\begin{tabular}{|p{1.5cm}||c|c||c|c||c|c|} \hline
\resizebox{1.9\columnwidth}{!}{
\begin{tabular}{lp{.2cm}ccp{.2cm}ccp{.2cm}cc} %\cline{2-10}
 & & \multicolumn{2}{c}{Al Kafi} & & \multicolumn{2}{c}{Al Istibsar} & &\multicolumn{2}{c}{Ibn Hanbal} \\ \cline{3-10}
 & & Simple & Refined & & Simple & Refined & & Simple  & Refined \\\bottomrule  %\hline \hline
Word count & &\multicolumn{2}{c}{98,943} & & \multicolumn{2}{c}{103,835} & & \multicolumn{2}{c}{20,354} \\ 
 Names      & & 20,473 & 12,060  & &20,646 &  14,613& & 4,762 & 3,013\\
Names/Narrator& &2.48 & 1.97 & & 2.17 &  1.84& & 1.49 & 1.25 \\
Narrators & &3,080 & 2,623 & & 6,164 &  5,767& & 2,082 & 1,755 \\ 
Narrators/Chain & & 5.28 & 4.84 & & 4.98&  4.76 & & 4.8 &4.05 \\
Chains & & 583 & 542 &  & 1,238& 1,211& & 433 & 433 \\ 
Ignored names & & 11,801 & 6,400 &  & 5,845 & 3,348 & & 1,386 & 642 \\ \hline
Segmentation accuracy & & 85\% & 96\%& & 96\%& 96\%& & 92\%& 92\%\\ 
Chain accuracy & & 99\% & 99\%&  & 98\%& 99\%& & 99\% & 97\% \\ 
%Narrator accuracy & & & & & &  & & & \\ 
Narrator name accuracy & & 93\% & 91\%& &  92\%& 90\% & & 90\% & 90\% \\ \hline
Name false positives & & 28\% & 7\%&  & 9\%& 4\% & & 34\% & 4\% \\ \hline
Sarf running time (secs.)& & 2.97 & 1.32 & & 2.93 & 1.31 & & .25 & .096\\ \hline \hline
 & & Buckwalter& ElixirFM & & Buckwalter & ElixirFM & & Buckwalter & ElixirFM \\\bottomrule
Names      & & 18,354 & 19,201 & & 17,934 &  18,202 & & 5,021 & 5,117 \\
Names/Narrator& & 2.6 & 2.1 & & 2.47 &  2.07 & & 1.83 & 1.62\\
Narrators & & 3,210 & 2,981 & & 6,425 &  6,208 & & 2,311 & 1,965 \\ 
Narrators/Chain & & 5.57 & 5.32 & & 5.46 &  5.32 & & 5.12 & 5.01 \\
Chains & & 627 & 601 &  & 1,318 & 1,250 & & 487 & 452 \\ 
Ignored names & & 9,642 & 9,915 &  & 6,118 & 6,874 & & 1,812 & 1,904 \\ \hline
Segmentation accuracy & & 76\%& 84\%& & 85\%& 89 \%& & 84\%& 85\%\\ 
Chain accuracy & & 78\% & 87\%&  & 87\%& 90\%& & 85\% & 86\% \\ 
%Narrator accuracy & & & & & &  & & & \\ 
Narrator name accuracy & & 76\% & 86\%& & 84\%& 87\% & & 84\% & 84\% \\ \hline
Name false positives & & 29\% & 22\%&  & 14\%& 13\% & & 36\% & 32\% \\ \hline
Running time (secs.)& & 6.65 & 2.78$\times60^2$& &4.55 & 2.3$\times60^2$ & & .66 & 29.2$\times$60 
\end{tabular}
}
\normalsize
\label{t:hadithresallresults}
\end{table*}

In this section we present results that compare Sarf to existing 
morphological analyzers such as Buckwalter and ElixirFM. 
We also evaluate our hypothesis of the utility of a case-based 
morphological analyzer.
We validate that the high-level analysis of the output 
of low-level morphological analysis can compensate for accuracy.
To achieve that we ran Sarf in two modes, simple and refined.
In the simple mode, we used Sarf as discussed in 
Section~\ref{sec:sarf} with the 
controller of Section~\ref{sec:controller}.
In the refined mode, we applied several low-level morphological 
refinements to Sarf such as disambiguation of 
female proper names so that the controller can ignore them. 
We also added a refinement that detected compound proper names 
such as \RL{`bd alr.hmn} 
as they are common in narrator names.
%Finally, we consider a possessive form as a reference to 
%a person and treat
%it as a narrator.

We ran our experiments using a dual core 2.66 Ghz 64-bit processor 
with 4GB of memory using a Linux operating system. 

We report our results in Table~\ref{t:hadithresallresults} on the 
three hadith books.
In general, we observe that the refinements slightly 
improved performance and accuracy at
the lowest level of abstraction by reducing false positives.
However, the refinements had little to no effect on the high 
accuracy measures at higher levels of abstraction.
We believe that this is a strong supporting evidence to our 
hypothesis.

The row for ``word count'' is an indicator of the size of the data.
The %numbers in 
rows ``names'', ``narrators'', and ``chains'' denote the total
number of the corresponding entity per book.
The relative magnitudes of these rows
reflect the hierarchical structure where 
a chain contains
narrators, and a narrator contains names.
The row ``ignored names'' reports the number of names detected 
outside the chains and thus are irrelevant to
the case study.
In addition, the rows ``names/narrator'' and ``narrators/chain'' 
report the average
number of names per narrator and 
the average number of narrators per chain respectively. 

The ``segmentation accuracy'' reports on the accuracy of 
segmentation of the book into separate hadiths.
The metric is computed manually over a representative sample 
of each book.
The same applies to all other accuracy measures.
The ``chain accuracy'' reflects the percentage of correct narrators 
covered by extracted chains.
The ``narrator name accuracy'' row reflects the percentage of 
correct proper names detected within
narrator structures.

The row ``name false positives'' reports the percentage of words 
recognized as names while they should not.
We notice that this low level metric improved dramatically 
with the refined analysis without substantially affecting the 
high abstract measures such as chain and segmentation
accuracy.

We notice that the controller in the simple analysis ignored 
more names and thus the case-based
controller compensated with a simple heuristic
for the inaccuracy of the low level 
analysis.
%and without the need of the low-level refinements.

We investigated the case of Al-Kafi where the accuracy of 
the simple analysis was significantly lower than that of the 
refined analysis. 
We discovered that the chains detected by the refined analysis 
were a subset of those detected by the simple
analysis. 
The reason for the lower accuracy is false positive chains, 
implying that the simple analysis did not miss important 
information. 
We hypothesize that a higher level of abstraction such as 
intersecting narrators from the hadith books with those from
the biographies can reduce the false positives in the chains.

%\subsection{Buckwalter and ElixirFM}
{\bf Buckwalter and ElixirFM.~~~}
We implemented our case study with Buckwalter in two phases. 
In phase one, we processed the entire text with Buckwalter and
obtained annotations to text. 
In the second phase, we fed the annotations to 
the hadith controller as if they were coming from Sarf. 
We measured the time it took to accomplish both tasks
ignoring the time spent streaming the annotations.
We did the same for ElixirFM. 

Sarf outperforms both Buckwalter and ElixirFM in terms of 
running time. 
It also produces higher accuracy measures 
mainly due to the extended
lexicon as well as to the fact that Sarf reduces false positive
names by ignoring affixes that can not form
names even if one possible solution of the stem is a name. 
%Doing the same with Buckwalter and ElixirFM requires  
%expanding the annotations resulting from one simple affix to be 
%reflected in all the composite affixes that contain it.
%expanding the annotations resulting from one affix to reflect
%its multiple composing affixes.
%However, this was easily performed using our recursive definition 
%of the affixes.

Buckwalter performed much better than ElixirFM in 
terms of running time, 
while ElixirFM outperformed Buckwalter in terms of accuracy.
%This is up to 
%our knowledge the first comparison of these two tools and 
We think the big difference is due to the sophistication of 
the formal functional morphological analysis of ElixirFM 
and its reliance on additional annotations extracted from 
both ATB~\cite{Maamouri:04} and the Prague Arabic Dependency Treebank~\cite{Prague04}.

\section{Conclusion and Future Work}
\label{sec:future}

We presented Sarf, a novel case-based morphological analyzer,
and validated its utility using the hadith literature
case study. 
Sarf uses non-deterministic finite state Automata with 
concatenative analysis and recursive affixes to analyze 
Arabic text. 
Results show that Sarf with the hadith controller
is faster and more accurate than
current morphological analyzers.


%TODO: merge into conclusion, taken from introduction
We make the following key contributions: 
\begin{enumerate}
\item {\bf Case-based analysis:}  We designed and implemented
a case-based morphological analyzer where a controller
guides the analysis. 
%\item {\bf Exhaustive analysis:} Since we have a case-based 
%controller that can decide on the fly whether an analysis is 
%accepted or not, we can afford to keep all valid analyses modulo
%those pruned by the case controller. 
%We do that using non-deterministic FSAs. 
\item {\bf Recursive affixes:}
We refine the concatenative analysis of 
Buckwalter~\shortcite{Buckwalter:02} and define affixes
recursively. 
This allows Sarf to perform better and 
maintain compositional part of speech (POS) tags.
\item {\bf Hadith case study:}
%We evaluated Sarf using the hadith case study. 
We were able to efficiently extract a complex three 
level hierarchical 
structure containing the chains of narrators 
with high accuracy. 
\end{enumerate}


In the future, 
we will try Sarf with other interesting case studies.
We plan to complete the hadith case study 
and automate narrator discovery and authentication in the 
biography books. 
%We plan to make use of the name connector and narrator
%connector stop phrases that our case study extracted to create a 
%learning system that can learn more names. 
%We also plan on developing an Arabic linguistic computational model 
%similar to that of ElixirFM~\cite{Otakar:07} but that can be 
%explored partially based on the case study. 

%\section{Acknoledgements}
%\label{sec:acc}
%We thank the Lebanese National Council for Scientific Research (LNCSR) for funding this research.


%For reasons of uniformity, Adobe's {\bf Times Roman} font should be
%used. In \LaTeX2e{} this is accomplished by putting

%\begin{quote}
%\begin{verbatim}
%\usepackage{times}
%\usepackage{latexsym}
%\end{verbatim}
%\end{quote}
%in the preamble.

%Additionally, it is of utmost importance to specify the {\bf
%  US-Letter format} (8.5in $\times$ 11in) when formatting the paper.
%When working with {\tt dvips}, for instance, one should specify {\tt
%  -t letter}.

%{\bf Citations}: Citations within the text appear
%in parentheses as~\cite{Gusfield:97} or, if the author's name appears in
%the text itself, as Gusfield~\shortcite{Gusfield:97}. 
%Append lowercase letters to the year in cases of ambiguities.  
%Treat double authors as in~\cite{Aho:72}, but write as 
%in~\cite{Chandra:81} when more than two authors are involved. 
%Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}.


%\section*{Acknowledgments}
% this will go into the regular submission

%\bibliographystyle{naaclhlt2010}
\bibliographystyle{acl}
{\small \bibliography{fzAr}}

\end{document}
