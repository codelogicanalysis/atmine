\section*{Reviewer 3 comments} 
\textit{Comments to the Author}



This work is an improvement step in a continuous previous work 
(Saref,MATAr) which leads to a complete project that can be 
used as a tool for other researchers in the field to help 
improving the Arabic language computations and processing.


It depends on the morphological analyser (Saref) results 
to define tags for flexible corpus annotation that is automatic 
but allow the users to correct and fix the tagging. 
In additions it extract entity and relational entities from 
the input document.


\begin{enumerate}[leftmargin=0mm,label=\bfseries CommentR3.\arabic*]
\item \label{Review.3.1}
As a reader, It wasn't very easy to followup with the writing 
style. 
I needed to return back to ``Saref'' and ``MATAr'' papers 
to understand the terminology used.

\answer{
  We cleared notations in the paper that might have led to 
    the confusion.
    For example, we cleared (1) the notation for morphological solutions,
    (2) the use of symbol $R$ as a both a relation tuple and a 
    tag set. 
    We clarified the \cci{isA} and \cci{contains} predicates. 
    We define the power set notation $2^A$ where $A$ is a set
    and $2^A$ is the set of all subsets of $A$.
}


\item \label{Review.3.2}
First, I think we can ask 
``What would a researcher reading this paper need to know?", the answer would be:

1- ``What is new in MERF?''. and this was clear for the reader 
    which is mainly
    a)(entity and relational entity extraction).
    b) defining tag of words based on a synonymic relation

\answer{
  Thank you. 
}

\item \label{Review.3.3}
2- ``How to use MERF?''
(you can have more explanations in your appendix and could have 
``help'' button in your API. This part was also relatively 
clear for the reader.

\answer{
  We will provide a GUI tutorial file on the tool's website. 
}


\item \label{Review.3.4}
3- The most important part is ``how MERF was built 
(methodology)?''. but I found this part was not very clear.

\answer{
  We added a methodology Section~\ref{sec:methodology} with a 
    new Figure~\ref{f:overview} that illustrates the four
    processes involved in the \framework Methodology. 
}

4-and for sure ``how it was evaluated and good results discussion''. 
which needs more attention.

\answer{
  We added more discussions to the results Section~\ref{sec:results}
    including case study size, development time evaluation, 
    complexity, and a discussion on required expertise. 
}

The following are some detailed recommendations:

\item \label{Review.3.5}
1-In the introduction, I think you need to omit 
    ``We discuss the importance of the morphological featureotivs supported in 
    MERF terms in Section 3; in brief, morphological preprocessing iskey to 
    Arabic NLP.'' , in page 2 since it will be repeated in page 5.

\answer{
  Done. 
}

\item \label{Review.3.6}
2- in the introduction you illustrated a target example, 
so why don you need the Motivation section. 
What about if combine both.

\answer{
  Thank you for the note. 
  We removed the motivation Section and embedded its content as 
  a running example in the \framework Components Section~\ref{sec:tool}. 
}


\item \label{Review.3.7}
In addition, at page 9 ``the edges and internal nodes are text, 
morphology-based, and word distance based relational entities'' 
is not clear.

\answer{
  We rephrased the sentence in the paper to read as follows. 
  \begin{quote}
The leaf nodes of the trees are matches to formulae and the internal nodes represent roots to sub-expression matches.
  \end{quote}
}


\item \label{Review.3.8}
please always define your terminology before using it and 
provide examples. suggest the ``formula" was not so clear. 
I suggest using regular grammar instead since you are using 
regular expression and the match tree can be considered as 
parse tree.

\answer{
  We modified the paper to refer to the collection of the regular
  expressions as a local grammar. 
  We also use matching parse trees instead of match trees in the paper now. 
  We added a sentence in the Introduction Section to 
  clarify that ``formula'' across the paper denotes a morphology
  based Boolean formula (MBF). 
  We do the same for regular expressions and rules. 
}


\item \label{Review.3.9}
2- Section ``Existing annotation and entity extraction tools” 
need to be with section 6 or section 6 need to be combined 
with it in as one section but leaving the comparison to 
the analysis section.

\answer{
We merged the content of Subsection 1.1 with the Related Work 
Section~\ref{sec:related}. 
}


\item \label{Review.3.10}
3- Section Background:Morphological analysis, 
the definition explained in page 9 did not match table 1 at page 7.

\answer{
  We modified the definition to indicate that the prefix and suffix 
  and the corresponding tags maybe concatenations of smaller prefix,
  suffix and tags.  
}


\item \label{Review.3.11}
4- Section 5 can be combined with section 2. 
you can introduce the user friendly interface using the 
specified example. 
Section 2 seemed not very important as stand alone section.

\answer{
  We removed the Motivation Section (2 in the old version) and
  embedded its contents in the \framework Components Section (4 in the
  old version and \ref{sec:tool} now). 
  We also introduced a Methodology Section~\ref{sec:methodology}. 
}


\item \label{Review.3.12}
a) figure 3 did not make it clear for the reader to 
understand the process. Also, the figure location is far 
away from its explanation.

\answer{
  We replaced the figure with Figure~\ref{f:overview}.
  Figure~\ref{f:overview} is 
  a clearer and is composed of four
  separate processes. 
  We discuss the new Figure in the Methodology Section~\ref{sec:methodology}.
}

\item \label{Review.3.13}
b) Syn$^k$ is nicely explained but I did not recognize it as 
your own idea. if this is your idea to use English translation 
as a pivot to extract Arabic synonyms then it needs to have 
more focus because I think this is new.

\answer{
Up to our knowledge, we introduced the idea. 
We claim that now as a novelty and a contribution in the introduction
and we rephrase the beginning of Section~\ref{subsec:synk}. 
}

\item \label{Review.3.14}
what is $2^{gloss}$ and $2^{s}$?

\answer{
The notation $2^s$ denotes the power set of set $s$ which is 
the set of all subsets of $s$. 
We clarify that in the paper. 
}


\item \label{Review.3.15}
please to make it easier for the reader to followup, 
let the example in figure 4 also be defined as the formula 
variables. w,{u},..so on

\answer{
We modified the paragraph explaining the example to refer
to the formula variables. 
}


\item \label{Review.3.16}
c) In MRE section,page 10, $k <=7$ why? 

\answer{
  We answer this by introducing the following in the paper.
  \begin{quote}
  We limit $k$ to a maximum of $7$ since we practically noticed that 
  (1) values above $7$ introduce significant semantic noise and
  (2) the computation is expensive without a bound. 
  \end{quote} 
}

\item \label{Review.3.17}
also, A belongs-to F , A is a morphological feature, 
then how CF belongs-to A. A is defined as one item not a set.

\answer{
F is a sets of sets. It is a set of morphological feature sets.
Each morphological feature set is in turn a set of values of that feature.
CF belongs to A denotes that CF is a value of a morphological feature
set  A. 
We clarify this point in SubSection~\ref{subsec:grammar}.
}


\item \label{Review.3.18}
MRT definition is not clear.

\answer{
  We modified the definition. We also 
  followed the MRE definition with a running illustrating example.
}



\item \label{Review.3.19}
"MRE" and "user defined relations and actions " sections 
were hard to be understood.

\answer{
  We provided a running example across both sections and rewrote parts of the sections. 
}

\item \label{Review.3.20}
MREF simulator: please provide examples soon after the 
definitions to make it easier for the reader. 
The reader can be lost so fast within the variable definitions. 

\answer{
We now provide running examples for the definitions in the 
MEF simulator Section. 
}


\item \label{Review.3.21}
for each word you are computing all the formulas values, 
so the complexity is O(text length X number of formulas), 
what is the range of number of formulas ?

\answer{
  The complexity is the number of morphological solutions 
  times the number of user defined formulae. 
  We provide no limit on the number of user defined formulae.
  Practically, in our case studies we did not need to defined
  more than 10 formulae per case study. 
  We include this discussion in the Results Section of the paper. 
}


\item \label{Review.3.22}
you are using R as tuple defining relation, 
then bellow you used it as set of feature vectors for the text. 
This is confusing the reader. 
I needed to read the definition in MATAr paper to understand 
what R is.

\answer{
  Thank you for pointing it out. We cleaned the definitions.
}


\item \label{Review.3.23}
d) In general, what was the data structure used? 
how would the annotated text be saved?

\answer{
  We included descriptions of the storage format 
  in the Methodology Section~\ref{sec:methodology}. 
}

\item \label{Review.3.24}
6- At section Results, comparing the development time 
did not strength the work. 
This is my first time to see researchers compare their 
algorithm in term of line of code and developing time. 
I would suggest comparing the algorithm complexity 
(linear , exponential, etc), and comparing how fast a user can 
accomplish similar jobs in the different applications. 
This can be done by assigning similar corpus annotation job 
and information extraction while computing time needed by 
the user 
``as a survey like comparison for qualitative evaluation''. 
you can search for ``user interface system evaluation''. 
There is heuristic evaluation where you give weight for job 
complexity and record time needed to complete it.

\answer{

We address the comment by adding the following at the beginning
of the Results Section~\ref{sec:results}. 

\begin{quote}
In this section we evaluate \framework with four case studies. 
We perform a survey like evaluation where developers manually 
built task specific information extraction tools for the case studies 
and other developers built equivalent \framework tools. 
The aim of the comparison is to showcase that \framework enables 
fast development of linguistic applications with similar accuracy 
and a reasonable affordable overhead in computational time. 
We report development time,
size of developed code versus size of grammar,
running time, and 
precision-recall as metrics
of cost, complexity, overhead, and accuracy,
respectively. 
\end{quote} 

Developers implemented the same tasks and ran on the same corpora. 
%
We also performed manual annotation to construct the golden reference. 
We did that by correcting the automated annotations of both annotators
for the benchmark corpora. 
We clarify that in the Results Section~\ref{sec:results}.
}


\item \label{Review.3.25}
Also, comparison with previous work done by (Zaraket ) 
in different version seems like comparing with your previous work 
only and not with others. 
It would be nice to compare others work too because user 
interface could be introduced in different view.

\answer{
  \framework targets entity and 
  relational entity extraction. 
  We are not aware and do not have access to other 
  previous work that performs morphology based relational entity 
  extraction for Arabic.
  We are open to perform such experiments in case the reviewers 
  can point us to such accessible tools. 
}

\item \label{Review.3.26}
What is the data size of your evaluation? 
I couldn't find how precision and recall was counted. 
you mentioned books but did you covered all the text in those 
books ? and how long was it? you can define it in term of 
number of relation extracted , tags added, expression processed, 
sentences,..etc

\answer{
  We address this in the Results Section~\ref{sec:results}
  as follows. 
  \begin{itemize} 
    \item 
  We include more information about the data and 
  the evaluation process.
\item 
  We include the size of the annotated data sets that 
  we used for accuracy evaluation in Table~\ref{tab:results}. 
\item 
  We now discuss how the golden reference was constructed. 
\item 
  We now discuss how precision and recall are computed as ratios of 
  entities correctly detected against extracted entities, and 
  entities correctly detected against the total number of entities, 
  respectively. 
\item 
  We also discuss the workload of the research assistant who built
  the task specific case studies. 
  \end{itemize} 
}


\item \label{Review.3.27}
7- At section Discussion, I strongly urge you to discuss 
WHY the precision and recall have these values.

\answer{
  We added  the following to the discussion Section. 
  \begin{quote}
We notice that ANGE, ATEEMA, and Genealogy tree report higher 
    precision than \framework. 
This is mainly due to their capacity 
to learn words and relations that may not have a match in the 
morphological analyzer based on co-occurrence relations. 
For example, the sequence $p_1 t_1 p_2$ where $p_1$ and 
$p_2$ are persons and $t_1$ is a tell relationship helps
indicate that $x$ is a tell relationship in $p_1 x p_2$ 
even if the morphological analyzer did not return the required
feature for $x$ to match a tell relationship. 
\framework does not have that capacity yet unless it is
encoded in the C++ actions. 
  \end{quote} 
}

\item \label{Review.3.28}
Clarity:  
It was not very clear how the work was done 
``Methodology part''. 
Section 4 ``MERF'' please add ``Methodology''.

\answer{
  We added a methodology section. 
}


\item \label{Review.3.29}
Having ``Existing annotation and entity extraction tools'' 
under unspecified section is misleading the reader. 
unnumbered subsections was misleading me as a reader. 

\answer{
  We fixed the non-numbered subsections.
  We removed Subsection ``Existing annotation and entity extraction tools''
  as requested in \ref{Review.3.9}.
}

\item \label{Review.3.30}
table 1 at page 7 while referring to it only at page 9. 
I think it needs to be closer to where it was mentioned in the text. 

\answer{
  Done.
}

\item \label{Review.3.31}
``MERF regular expressions support operators such as concatenation, zero or one, zero or more, one or more, up to $M$” what is M? 
it could be for example 0<=M<=|chunk| 


\answer{
  We clarify the 
  notation ``up to $M$'' in the paper to be as follows. 
  \begin{quote}
   up to $M$ repetitions where $M$ is 
   a non-zero positive integer 
  \end{quote}
}


\item \label{Review.3.32}
Correctness: 
1- in the abstract, the sentence ``These techniques and 
tools require expertise in linguistics and programming and 
lack support of Arabic morphological analysis which is key to 
process Arabic text''
gives the meaning that ``it require lack support'' so I think it 
needs to be reformulated 

\answer{
  We rephrased the sentence.
}


\item \label{Review.3.33}
2-``defines tag types and''.// unify the font here and 
many other places in the paper. 
for example look at words(,correct,commercial)

\answer{
  We used italics to emphasize important 
  concepts that we introduce for the first time. 
  We reduced the use of emphasized text in the paper. 
}

\item \label{Review.3.34}
Originality:  
This work will contribute in facilitating Arabic corpus 
annotation, if it was made available for other researchers in 
the field to access and use it, although there is no 
referencing to where it can be accessed. 
this is an improvement of a per-existing work ``MATAr: Morphology-based Tagger for Arabic''

\answer{
\framework is open source and available online. We modified the
contributions in the Introduction Section to indicate that. 
}


\end{enumerate}
